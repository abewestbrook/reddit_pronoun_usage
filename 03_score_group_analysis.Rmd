---
title: "03_score_group_analysis"
output: html_notebook
---
## Define Large Subreddits

```{r}
large_subreddits <- pronoun_use_by_score_group %>%
  filter(
    score_group == "z1000+"
  ) %>% select(subreddit)
large_subreddits <- large_subreddits[,1,drop = TRUE]


```


##Density plots of pronoun use

```{r}
library(plotly)

plot_score_group = function (a,pronoun) {
  ggplotly(ggplot(a,aes_string(x = paste("contains_",tolower(pronoun),sep=""),color = "score_group")) + geom_density()+
             facet_wrap(~topic_level1)
             )}






large_sub_pronoun_use <- pronoun_use_by_score_group %>%
  filter(
    !subreddit %in% large_subreddits
  ) 


plot_score_group(large_sub_pronoun_use,"I") 
plot_score_group(large_sub_pronoun_use,"We") 
plot_score_group(large_sub_pronoun_use,"They")


```

```{r}



agg_pronoun_use <- pronoun_use_by_score_group %>%
  filter(grepl("Sports", all_topics)) %>%
  group_by(score_group,subreddit,all_topics) %>%
  summarise(
    agg_i      = sum(num_comments*ZNcontains_i     )/sum(num_comments),
    agg_me     = sum(num_comments*ZNcontains_me    )/sum(num_comments),
    agg_we     = sum(num_comments*ZNcontains_we    )/sum(num_comments),
    agg_us     = sum(num_comments*ZNcontains_us    )/sum(num_comments),
    agg_they   = sum(num_comments*ZNcontains_they  )/sum(num_comments),
    agg_them   = sum(num_comments*ZNcontains_them  )/sum(num_comments),
    agg_you    = sum(num_comments*ZNcontains_you   )/sum(num_comments),
    
    total_comments = sum(num_comments),
    
    unweighted_i      =  mean(ZNcontains_i   ),
    unweighted_me     =  mean(ZNcontains_me  ),
    unweighted_we     =  mean(ZNcontains_we  ),
    unweighted_us     =  mean(ZNcontains_us  ),
    unweighted_they   =  mean(ZNcontains_they),
    unweighted_them   =  mean(ZNcontains_them),
    unweighted_you    =  mean(ZNcontains_you )
            
    
  )# %>% arrange(t1_t2) 
plot_categories_score_groups = function(agg_pronoun_use,pronoun){
ggplotly(
  ggplot(agg_pronoun_use, aes_string(x = "score_group", y = paste("agg_",tolower(pronoun),sep=""), name = "subreddit", color = "all_topics")) +
  geom_point(aes(size = total_comments))
)
  }

plot_categories_score_groups(agg_pronoun_use,"I")
plot_categories_score_groups(agg_pronoun_use,"Me")
plot_categories_score_groups(agg_pronoun_use,"We")
plot_categories_score_groups(agg_pronoun_use,"Us")
plot_categories_score_groups(agg_pronoun_use,"They")
plot_categories_score_groups(agg_pronoun_use,"Them")
plot_categories_score_groups(agg_pronoun_use,"You")





```



## Analysis by score (not group)

#Base rates across subreddits


```{r}
pronoun_source <- collective_pronoun_by_score %>%
  filter(score < 50) #%>%
  #filter(grepl("Lifestyle", all_topics)) 

#for pronoun in c("individual", "collective")
ggplotly(
  ggplot(pronoun_source, aes_string(x = "score", y = "ZNcollective",
                                                 name = "subreddit")) + geom_smooth(aes(size = num_comments))
  )

ggplotly(
  ggplot(pronoun_source, aes_string(x = "score", y = "ZNindividual",
                                                 name = "subreddit", color = "all_topics")) + geom_smooth(aes(size = num_comments))
  )
```

large subs

```{r}
large_subreddits <- collective_pronoun_by_score %>%
  filter(
    score >1000, num_comments > 10
  ) %>% select(subreddit)
large_subreddits <- large_subreddits[,1,drop = TRUE]

```


##POT6
```{r}
collective_pronoun_by_score %>%
  filter(subreddit %in% large_subreddits) %>%
  #filter(grepl("Soccer", all_topics)) %>%
  filter(score<25) %>%
  filter(!topic_level1 %in% c("Adult and NSFW","Architecture","Art"),
         !is.na(topic_level1)) %>%
  group_by(score
           ,topic_level1
           ) %>%
  summarise(
    collective = sum(ZNcollective*num_comments)/sum(num_comments),
    individual = sum(ZNindividual*num_comments)/sum(num_comments),
    num_comments = sum(num_comments)
  ) %>% 
  ggplot()+ geom_hline(yintercept = 0) + geom_vline(xintercept = 1) +
  geom_point(aes(score,collective,color = "collective",size = num_comments))+
  geom_point(aes(score,individual,color = "individual",size = num_comments))+
  theme_classic() +
  facet_wrap(~topic_level1) +
  scale_y_continuous(limits = c(-20,50))+
  labs(title = "Pronoun Use by Score of the Comment in Different Categories of Subreddits" ,
       subtitle = "Pronoun use normalized based on comments with score 1",
       caption = "") + xlab("Comment Score") + ylab("Percentage change in the proportion of comments with pronoun\n (compared to comments with score 1)") 

collective_pronoun_by_score %>%
  filter(subreddit %in% large_subreddits) %>%
  #filter(grepl("Soccer", all_topics)) %>%
  filter(score<25) %>%
  filter(!topic_level1 %in% c("Adult and NSFW","Architecture","Art"),
         !is.na(topic_level1)) %>%
  group_by(score
           #,topic_level1
           ) %>%
  summarise(
    collective = sum(ZNcollective*num_comments)/sum(num_comments),
    individual = sum(ZNindividual*num_comments)/sum(num_comments),
    num_comments = sum(num_comments)
  ) %>% 
  ggplot()+ geom_hline(yintercept = 0) + geom_vline(xintercept = 1) +
  geom_point(aes(score,collective,color = "collective",size = num_comments))+
  geom_point(aes(score,individual,color = "individual",size = num_comments))+
  theme_classic() +
  #facet_wrap(~topic_level1) +
  scale_y_continuous(limits = c(-20,50))#+
 # scale_x_log10()


#worldnews

collective_pronoun_by_score %>%
  filter(subreddit == "worldnews") %>%
  #filter(grepl("Soccer", all_topics)) %>%
  filter(score<25) %>%
  filter(!topic_level1 %in% c("Adult and NSFW","Architecture","Art"),
         !is.na(topic_level1)) %>%
  group_by(score
           #,topic_level1
           ) %>%
  summarise(
    collective = sum(ZNcollective*num_comments)/sum(num_comments),
    individual = sum(ZNindividual*num_comments)/sum(num_comments),
    num_comments = sum(num_comments)
  ) %>% 
  ggplot()+ geom_hline(yintercept = 0) + geom_vline(xintercept = 1) +
  geom_point(aes(score,collective,color = "collective",size = num_comments))+
  geom_point(aes(score,individual,color = "individual",size = num_comments))+
  theme_classic() +
  #facet_wrap(~topic_level1) +
  scale_y_continuous(limits = c(-20,50))#+
```

```{r}
weird <- scales::trans_new("signed_log",
       transform=function(x) sign(x)*log(abs(x)),
       inverse=function(x) sign(x)*exp(abs(x)))

collective_pronoun_by_score %>%
  #filter(grepl("Sports", all_topics)) %>%
  filter(score<100, score > 0)%>%
  filter(!topic_level1 %in% c("Adult and NSFW","Architecture","Art"),
         !is.na(topic_level1)) %>%
  group_by(score
           #, topic_level1
           )%>%
  summarise(
    num_comments = sum(num_comments)
    
    
  ) %>%
  ggplot()+ 
  geom_point(aes(score,num_comments))+ scale_y_log10(name = "Number of comments per score (log10 scale)" ,breaks = c(1000000,10000000,100000000,1000000000),labels = scales::comma)+ scale_x_continuous(trans = weird, name = "Score (log10 scale)",breaks = c(1,5,10,25,50,75,100),labels = scales::comma) + theme_linedraw()
```
Size of subreddit vs. score-pronoun relation

```{r}

collective_pronoun_by_score %>%
  filter(score == 50) %>%
  ggplot(aes(num_comments,ZNcollective))+geom_point()+
  #geom_smooth()+
  scale_x_log10()+scale_y_continuous(limits = c(-100,250))+ facet_wrap(~ topic_level1)
```

