---
title: "comments_over_time"
output: html_notebook
---

```{r}
library(tidyverse)
```

load data
```{r}
library(tidyverse)
library(bigrquery)

project_id <- "redditcomments-197501" # put your project ID here


sql <- "
#standardSQL
  WITH pronoun_usage AS (
  SELECT
    author,
    subreddit,
    created_utc,
    REGEXP_CONTAINS(body,
      '(\bI\b)') contains_i,
    REGEXP_CONTAINS(body,
      '(?i)me(?-i)') contains_me,
    REGEXP_CONTAINS(body,
      '(?i)we(?-i)') contains_we,
    REGEXP_CONTAINS(body,
      '\bus\b') contains_us,
    REGEXP_CONTAINS(body,
      '(?i)they(?-i)') contains_they,
    REGEXP_CONTAINS(body,
      '(?i)I(?-i)') contains_them,
    body
  FROM(
    SELECT
      *
    FROM
      `fh-bigquery.reddit_comments.2016*`)
    
  WHERE
    subreddit = 'The_Donald' ),
  first_post AS (
  SELECT
    author,
    subreddit,
    MIN(created_utc) AS first_post_utc
  FROM
    pronoun_usage
  GROUP BY
    author,
    subreddit )
SELECT
  a.author,
  a.subreddit,
  a.created_utc,
  b.first_post_utc,
  a.contains_i,
  a.contains_me,
  a.contains_we,
  a.contains_us,
  a.contains_they,
  a.contains_them
FROM
  pronoun_usage a
LEFT JOIN
  first_post b
ON
  a.author=b.author
  AND a.subreddit=b.subreddit
"

# Execute the query and store the result
reddit_comments_time <- query_exec(sql, project = project_id, max_pages = Inf, use_legacy_sql = FALSE)
```

## Alternate load from google storage bucket

```{r}
library(readr)
reddit_comments_time <- read_csv("pronoun_usage.csv", progress = FALSE)
```

```{r}
library(readr)
reddit_comments_time <- read_csv("pronoun_usage_2016_v2", progress = FALSE)
```

## Create new variables

```{r}
library(tidyverse)

reddit_comments_time <- reddit_comments_time %>%
  mutate(
    days_since_fp = (created_utc - first_post_utc)/60/60/24,
    days_since_jan1 = (created_utc - min(created_utc))/60/60/24
    )
```



```{r}
reddit_comments_time %>%
  sample_n(10^6) %>%
  ggplot(aes()) +
  geom_smooth(aes(x = days_since_fp, y=contains_i   , color = "FP sing" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_me  , color = "FP sing" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_we  , color = "FP plur" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_us  , color = "FP plur" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_they, color = "TP plur" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_them, color = "TP plur" )) 
  

```

##Rescale variables to same intial usage rates (1)

```{r}
library(tidyverse)
var_offsets_time <- reddit_comments_time %>%
  filter(days_since_fp < 10, days_since_fp > 1) %>%
  summarise(
    contains_i_offset   = mean(contains_i   ),
    contains_me_offset  = mean(contains_me  ),
    contains_we_offset  = mean(contains_we  ),
    contains_us_offset  = mean(contains_us  ),
    contains_they_offset= mean(contains_they),
    contains_them_offset= mean(contains_them)
  )


```

```{r}

reddit_comments_time %>%
  mutate(
    contains_i    = contains_i   / var_offsets$contains_i_offset,
    contains_me   = contains_me  / var_offsets$contains_me_offset,
    contains_we   = contains_we  / var_offsets$contains_we_offset,
    contains_us   = contains_us  / var_offsets$contains_us_offset,
    contains_they = contains_they/ var_offsets$contains_they_offset,
    contains_them = contains_them/ var_offsets$contains_them_offset 
    ) %>%
  filter(author %in% committed_authors$author) %>%
  ggplot(aes()) +
  geom_smooth(aes(x = days_since_fp, y=contains_i   , color = "I"    )) +
  geom_smooth(aes(x = days_since_fp, y=contains_me  , color = "Me"   )) +
  geom_smooth(aes(x = days_since_fp, y=contains_we  , color = "We"   )) +
  geom_smooth(aes(x = days_since_fp, y=contains_us  , color = "Us"   )) +
  geom_smooth(aes(x = days_since_fp, y=contains_they, color = "They" )) +
  geom_smooth(aes(x = days_since_fp, y=contains_them, color = "Them" )) 
  

```

##Comments over the course of 2016
# Committed authors (between 50 and 1000 total comments)
```{r}
committed_authors <- reddit_comments_time %>%
  group_by(author) %>%
  summarise(total_comments = n()) %>%
  filter(total_comments > 5, total_comments < 10000)

```
```{r}
reddit_comments_time %>%
filter(author %in% committed_authors$author) %>%
  ggplot(aes(x = days_since_jan1)) +
  geom_smooth(aes(y=contains_i   , color = "I"   )) +
  geom_smooth(aes(y=contains_me  , color = "Me"  )) +
  geom_smooth(aes(y=contains_we  , color = "We"  )) +
  geom_smooth(aes(y=contains_us  , color = "Us"  )) +
  geom_smooth(aes(y=contains_they, color = "They" )) +
  geom_smooth(aes(y=contains_them, color = "Them" )) 
```



##By order of comments (#of posts in the subreddit)

```{r}


var_offsets_order <- reddit_comments_time %>%
  filter(author %in% committed_authors$author) %>%
    group_by(author) %>%
  mutate(post_num = rank(created_utc)) %>%
  ungroup() %>%
  filter(post_num < 10) %>%
  summarise(
    contains_i_offset   = mean(contains_i   ),
    contains_me_offset  = mean(contains_me  ),
    contains_we_offset  = mean(contains_we  ),
    contains_us_offset  = mean(contains_us  ),
    contains_they_offset= mean(contains_they),
    contains_them_offset= mean(contains_them)
  )
```

```{r}
reddit_comments_time %>%
  filter(author %in% committed_authors$author) %>%
    group_by(author) %>%
  mutate(post_num = rank(created_utc)) %>%
  ungroup() %>%
  mutate(
    contains_i    = contains_i   / var_offsets_order$contains_i_offset,
    contains_me   = contains_me  / var_offsets_order$contains_me_offset,
    contains_we   = contains_we  / var_offsets_order$contains_we_offset,
    contains_us   = contains_us  / var_offsets_order$contains_us_offset,
    contains_they = contains_they/ var_offsets_order$contains_they_offset,
    contains_them = contains_them/ var_offsets_order$contains_them_offset 
    ) %>%
  filter(post_num < 2000) %>%
  #filter(author %in% committed_authors$author) %>%
  #sample_n(10^6) %>%
  ggplot(aes(x = post_num)) +
  geom_smooth(aes(y=contains_i   , color = "I"    )) +
  geom_smooth(aes(y=contains_me  , color = "Me"   )) +
  geom_smooth(aes(y=contains_we  , color = "We"   )) +
  geom_smooth(aes(y=contains_us  , color = "Us"   )) +
  geom_smooth(aes(y=contains_they, color = "They" )) +
  geom_smooth(aes(y=contains_them, color = "Them" )) +
  scale_x_log10()
```




```{r}
reddit_comments_time %>%
  filter(author %in% committed_authors$author) %>%
    group_by(author) %>%
  mutate(post_num = rank(created_utc)) %>%
  ungroup() %>%
  filter(post_num%%1 == 0) %>%
  group_by(post_num) %>%
  summarise(
    contains_i    = mean(contains_i   ),
    contains_me   = mean(contains_me  ),
    contains_we   = mean(contains_we  ),
    contains_us   = mean(contains_us  ),
    contains_they = mean(contains_they),
    contains_them = mean(contains_them),
    average_use   = mean(contains_i+contains_me+contains_them+contains_we+contains_us+contains_they)/6,
    num_authors = n()
  ) %>%
  filter(post_num < 40) %>%
  mutate(
    contains_i    = 100*(contains_i   /(   .$contains_i[1]*.5+   .$contains_i[2]*.25+   .$contains_i[3]*.25 )-1),
    contains_me   = 100*(contains_me  /(  .$contains_me[1]*.5+  .$contains_me[2]*.25+  .$contains_me[3]*.25 )-1),
    contains_we   = 100*(contains_we  /(  .$contains_we[1]*.5+  .$contains_we[2]*.25+  .$contains_we[3]*.25 )-1),
    contains_us   = 100*(contains_us  /(  .$contains_us[1]*.5+  .$contains_us[2]*.25+  .$contains_us[3]*.25 )-1),
    contains_they = 100*(contains_they/(.$contains_they[1]*.5+.$contains_they[2]*.25+.$contains_they[3]*.25 )-1),
    contains_them = 100*(contains_them/(.$contains_them[1]*.5+.$contains_them[2]*.25+.$contains_them[3]*.25 )-1),
    average_use   = 100*(average_use  /(.$average_use[1]*.5+.$average_use[2]*.25+.$average_use[3]*.25 )-1)
    ) %>%
  #sample_n(10^6) %>%
  ggplot(aes(x = post_num)) +
  geom_point(aes(y=contains_i   , color = "I"    , size = num_authors)) +
  geom_point(aes(y=contains_me  , color = "Me"   , size = num_authors)) +
  geom_point(aes(y=contains_we  , color = "We"   , size = num_authors)) +
  geom_point(aes(y=contains_us  , color = "Us"   , size = num_authors)) +
  geom_point(aes(y=contains_they, color = "They" , size = num_authors)) +
  geom_point(aes(y=contains_them, color = "Them" , size = num_authors)) +
  geom_point(aes(y=average_use  , color = "Average" , size = num_authors)) +
  geom_smooth(aes(y=contains_i   , color = "I"    )) +
  geom_smooth(aes(y=contains_me  , color = "Me"   )) +
  geom_smooth(aes(y=contains_we  , color = "We"   )) +
  geom_smooth(aes(y=contains_us  , color = "Us"   )) +
  geom_smooth(aes(y=contains_they, color = "They" )) +
  geom_smooth(aes(y=contains_them, color = "Them" )) +
  geom_smooth(aes(y=average_use  , color = "Average" )) +
  scale_x_continuous(name = "Nth post in given subreddit for every author ") +
  scale_y_continuous(name = "% change in pronoun use (dots = average across authors)")
```


```{r}
reddit_comments_time %>%
  group_by(author) %>%
  mutate(post_num = rank(created_utc)) %>%
  head(10)
```


```{r}
library(plotly)

  ggplot(reddit_comments_time,aes(post_num))+
  geom_histogram(binwidth = 1) + 
    scale_x_continuous(limits = c(0,100)+
                         scale_y_log10(limits = c(1,10000)))

```


```{r}
reddit_comments_time %>%
  group_by(author) %>%
  summarise(total_comments = n()) %>%
  ggplot()+
  geom_density(aes(total_comments))+
  scale_x_continuous(limits = c(0,100))

reddit_comments_time %>%
  group_by(author) %>%
  summarise(total_comments = n()) %>%
  ggplot()+
  stat_ecdf(aes(total_comments))+
  scale_x_continuous(limits = c(0,100))
  
```



```{r}
reddit_comments_time %>%
  group_by(author) %>%
  summarise(
   total_comments =  n()
  ) %>%
  filter(total_comments > 1000) %>%
  arrange(desc(total_comments))
#%>%
 # ggplot() + geom_histogram(aes(total_comments))
  
```


```{r}
reddit_comments_time %>%
  sample_n(100000)%>%
  ggplot(aes(days_since_jan1, days_since_fp))+
    geom_point()+
  stat_density_2d(aes(fill = ..level..), geom = "polygon")
```

