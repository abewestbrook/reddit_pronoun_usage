---
title: "comments_by_post_num"
output: html_notebook
---
```{r}
library(tidyverse)
library(bigrquery)

project_id <- "redditcomments-197501" # put your project ID here


sql <- "#standardSQL
WITH
  pronoun_usage AS (
  SELECT
    author,
    subreddit,
    ROW_NUMBER() OVER(PARTITION BY author, subreddit ORDER BY created_utc ASC) post_num,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bi\b)') THEN 1
      ELSE 0
    END AS contains_i,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bme\b)') THEN 1
      ELSE 0
    END AS contains_me,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bwe\b)') THEN 1
      ELSE 0
    END AS contains_we,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bus\b)') THEN 1
      ELSE 0
    END AS contains_us,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bthey\b)') THEN 1
      ELSE 0
    END AS contains_they,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\bthem\b)') THEN 1
      ELSE 0
    END AS contains_them,
    CASE
      WHEN REGEXP_CONTAINS(LOWER(body), r'(\byou\b)') THEN 1
      ELSE 0
    END AS contains_you
  FROM
    `fh-bigquery.reddit_comments.20*`
  WHERE
     subreddit IN (
        'The_Donald',
       'TwoXChromosomes',
       'GenderCritical',
       'AskMen',
       'AskWomen',
       'AskReddit',
       'ainbow',
       'science',
       'Futurology',
       'chemicalreactiongifs',
       'personalfinance',
       'Jokes',
       'Showerthoughts',
       'AskReddit',
       'news',
       'funny',
       'Buddhism',
       'Anarchism',
       'vagabond',
       'Meditation',
       'awakened',
       'LateStageCapitalism',
       'Conservative',
       'Political_Revolution',
       'soccer',
       'Gunners',
       'AdviceAnimals',
       'worldnews',
       'atheism',
       'exmormon'
       
      
       ) AND
    author NOT IN ('deleted')),
  first_post AS (
  SELECT
    author,
    subreddit,
    COUNT(*) AS total_num_comments
  FROM
    pronoun_usage
  GROUP BY
    author,
    subreddit )
SELECT
  a.subreddit subreddit,
  a.post_num post_num,
  COUNT(b.author) num_authors,
  AVG(a.contains_i) contains_i,
  AVG(a.contains_me) contains_me,
  AVG(a.contains_we) contains_we,
  AVG(a.contains_us) contains_us,
  AVG(a.contains_they) contains_they,
  AVG(a.contains_them) contains_them,
  AVG(a.contains_you) contains_you
FROM
  pronoun_usage a
LEFT JOIN
  first_post b
ON
  a.author=b.author
  AND a.subreddit=b.subreddit
WHERE
  total_num_comments >= 1
  AND post_num <= 1000
GROUP BY
  post_num,
  subreddit
HAVING
  num_authors > 100

"
sql <- "SELECT * FROM `redditcomments-197501.authors.pronoun_use_all_all_post_num`"
# Execute the query and store the result
reddit_comments_post_num <- query_exec(sql, project = project_id, max_pages = Inf, use_legacy_sql = FALSE)




```





```{r}
library(readr)
library(tidyverse)
reddit_comments_post_num <- read_csv("results-20181020-124141.csv")

```



```{r}
start_points <- reddit_comments_post_num %>%
  filter(post_num < 5, post_num >1) %>%
  group_by(subreddit) %>%
  summarise(
    offset_i           = mean(contains_i   ),
    offset_me          = mean(contains_me  ),
    offset_we          = mean(contains_we  ),
    offset_us          = mean(contains_us  ),
    offset_they        = mean(contains_they),
    offset_them        = mean(contains_them),
    offset_you         = mean(contains_you ),
    offset_average_use = mean(contains_i+contains_me+contains_them+contains_we+contains_us+contains_they+contains_you)/7
  )
```
```{r echo = FALSE, message = FALSE}
reddit_comments_post_num %>%
  left_join(start_points, by = c("subreddit" = "subreddit")) %>%
  filter(post_num <50, 
        # !subreddit %in% c("chemicalreactiongifs", "vagabond", "Meditation")) %>%
          subreddit %in% c("The_Donald", "LateStageCapitalism")) %>%
  arrange(sort(post_num)) %>%
  group_by(subreddit) %>%
  mutate(
    contains_i    = 100*(contains_i   /(offset_i   )-1),
    contains_me   = 100*(contains_me  /(offset_me  )-1),
    contains_we   = 100*(contains_we  /(offset_we  )-1),
    contains_us   = 100*(contains_us  /(offset_us  )-1),
    contains_they = 100*(contains_they/(offset_they)-1),
    contains_them = 100*(contains_them/(offset_them)-1),
    contains_you = 100*(contains_you  /(offset_you )-1),
    average_use   = 100*(((contains_i+contains_me+contains_them+contains_we+contains_us+contains_they+contains_you)/7/offset_average_use -1) ) )%>%
  ggplot(aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_point(aes(y=contains_i   , color = "I"      )) + # , size = num_authors)) +
geom_point(aes(y=contains_me  , color = "Me"     )) + # , size = num_authors)) +
geom_point(aes(y=contains_we  , color = "We"     )) + # , size = num_authors)) +
geom_point(aes(y=contains_us  , color = "Us"     )) + # , size = num_authors)) +
geom_point(aes(y=contains_they, color = "They"   )) + # , size = num_authors)) +
geom_point(aes(y=contains_them, color = "Them"   )) + # , size = num_authors)) +
geom_point(aes(y=contains_you , color = "You"    )) + # , size = num_authors)) +
#geom_point(aes(y=average_use  , color = "Average")) + # , size = num_authors)) +
geom_smooth(aes(y=contains_i   , color = "I"      ), se = TRUE) +
geom_smooth(aes(y=contains_me  , color = "Me"     ), se = TRUE) +
geom_smooth(aes(y=contains_we  , color = "We"     ), se = TRUE) +
geom_smooth(aes(y=contains_us  , color = "Us"     ), se = TRUE) +
geom_smooth(aes(y=contains_they, color = "They"   ), se = TRUE) +
geom_smooth(aes(y=contains_them, color = "Them"   ), se = TRUE) +
geom_smooth(aes(y=contains_you,  color = "You"    ), se = TRUE) +
#geom_smooth(aes(y=average_use  , color = "Average"), se = FALSE) +
  facet_wrap(~subreddit) + 
  scale_colour_manual(
    "",
    breaks = c(
      #"Average",
      "I"      ,
      "Me"     ,
      "Them"   ,
      "They"   ,
      "Us"     ,
      "We"     ,
      "You"
    ),
    values = c( #"dark gray",
                "dark blue","light blue", "pink", "red", "light green", "dark green", "yellow"))
```
Single pronoun

```{r}
library(plotly)
a <- reddit_comments_post_num %>%
  left_join(start_points, by = c("subreddit" = "subreddit")) %>%
  filter(post_num <50, 
         !subreddit %in% c("chemicalreactiongifs", "personalfinance")) %>%
        #  subreddit %in% c("The_Donald", "LateStageCapitalism")
        #) %>%
  arrange(sort(post_num)) %>%
  group_by(subreddit) %>%
  mutate( contains_i    = 100*(contains_i   /(offset_i   )-1),
    contains_me   = 100*(contains_me  /(offset_me  )-1),
    contains_we   = 100*(contains_we  /(offset_we  )-1),
    contains_us   = 100*(contains_us  /(offset_us  )-1),
    contains_they = 100*(contains_they/(offset_they)-1),
    contains_them = 100*(contains_them/(offset_them)-1),
    contains_you = 100*(contains_you  /(offset_you )-1),
    average_use   = 100*(((contains_i+contains_me+contains_them+contains_we+contains_us+contains_they+contains_you)/7/offset_average_use -1) ) )
  
ggplotly(
ggplot(a,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_we   , color = subreddit), se = FALSE)  )
ggplotly(
ggplot(a,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_i   , color = subreddit), se = FALSE)  )
ggplotly(
ggplot(a,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_they   , color = subreddit), se = FALSE)  )
ggplotly(
ggplot(a,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_you   , color = subreddit), se = FALSE)  )
```

```{r}
a1<-a %>% filter(post_num == 49)

ggplotly(
ggplot(a1,aes(num_authors,contains_you,color=subreddit)) +
  geom_point()+geom_smooth(method = "lm") +
  scale_x_log10()
)
  
```



50th post pronounc change

```{r}
post_num_50_change <- read_csv("results-20181020-143724.csv")
```
```{r}
post_num_50_change %>%
  sample_n(5000) %>%
ggplot(aes(num_authors,contains_we)) +
  geom_point(aes(size = num_authors))+
  geom_smooth()+
  scale_y_continuous(limits = c(-100,100))+
  scale_x_log10()
```



##All Subreddits, all post num < 500 & num authors > 20


##Load Data



```{r}

library(readr)
library(tidyverse)


pronoun_by_post_num <- read_csv("pronoun_by_post_num_min50.csv") %>%
  left_join(read_csv("subreddit_categories.csv"), by = c("subreddit" = "subreddit"))

start_points_all_subs <- pronoun_by_post_num %>%
  filter(post_num  == 1) %>%
  group_by(subreddit) %>%
  summarise(
    offset_i           = mean(contains_i   ),
    offset_me          = mean(contains_me  ),
    offset_we          = mean(contains_we  ),
    offset_us          = mean(contains_us  ),
    offset_they        = mean(contains_they),
    offset_them        = mean(contains_them),
    offset_you         = mean(contains_you ),
    offset_average_use = mean(contains_i+contains_me+contains_them+contains_we+
                                contains_us+contains_they+contains_you)/7
  )

pronoun_by_post_num <- pronoun_by_post_num %>%
  left_join(start_points_all_subs, by = c("subreddit" = "subreddit")) %>%
  
      #   !subreddit %in% c("chemicalreactiongifs", "personalfinance")) %>%
        #  subreddit %in% c("The_Donald", "LateStageCapitalism")
        #) %>%
  arrange(sort(post_num)) %>%
  group_by(subreddit) %>%
  mutate( 
    ZNcontains_i    = 100*(contains_i   /(offset_i   )-1),
    ZNcontains_me   = 100*(contains_me  /(offset_me  )-1),
    ZNcontains_we   = 100*(contains_we  /(offset_we  )-1),
    ZNcontains_us   = 100*(contains_us  /(offset_us  )-1),
    ZNcontains_they = 100*(contains_they/(offset_they)-1),
    ZNcontains_them = 100*(contains_them/(offset_them)-1),
    ZNcontains_you  = 100*(contains_you  /(offset_you )-1),
    ZNaverage_use   = 100*(((contains_i+contains_me+contains_them+
                          contains_we+contains_us+contains_they+contains_you)
                          /7/offset_average_use -1)),
    
    all_topics = paste(topic_level1,coalesce(topic_level2,""),coalesce(topic_level3,""),sep="-")
    
    )
```

Number of authors across subreddits analyzed

```{r}

pronoun_by_post_num %>%
  filter(post_num == 50) %>%
  nrow(.)

pronoun_by_post_num %>%
  filter(post_num == 50) %>%
  ggplot()+
  stat_ecdf(aes(num_authors)) +
  scale_x_log10()
```




```{r}

```


```{r}
library(plotly)

subreddit_list <- pronoun_by_post_num %>% 
  filter(post_num == 50, num_authors > 50) %>% 
  select(subreddit)
  
pronoun_by_post_num_short <- left_join(subreddit_list, filter(pronoun_by_post_num, post_num<51), by = c("subreddit" = "subreddit") )
plot_pronouns = function(pronoun_by_post_num,t1_t2) {
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_we   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_us   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_i   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_me   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_they   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_them   , color = t1_t2), se = FALSE)  )
ggplotly(
ggplot(pronoun_by_post_num_short,aes(post_num)) +
  geom_hline(yintercept = 0) + 
geom_smooth(aes(y=contains_you   , color = t1_t2), se = FALSE)  )
}

plot_pronouns(pronoun_by_post_num,topic_level1)

```


##Pronoun change by initial use rate 
# by subreddit



```{r}
n50_pronoun_change <- pronoun_by_post_num %>%
  filter(num_authors > 60,post_num >40, post_num < 60) %>%
  group_by(subreddit) %>%
  summarise(
    n50_i           = mean(ZNcontains_i   ),
    n50_me          = mean(ZNcontains_me  ),
    n50_we          = mean(ZNcontains_we  ),
    n50_us          = mean(ZNcontains_us  ),
    n50_they        = mean(ZNcontains_they),
    n50_them        = mean(ZNcontains_them),
    n50_you         = mean(ZNcontains_you ),
    n50_author      = mean(num_authors)
  )  %>% ungroup()
n50_pronoun_change  <- pronoun_by_post_num %>%
  filter(post_num == 50) %>%
  left_join(n50_pronoun_change, by = c("subreddit"="subreddit")) %>%
  ungroup() #%>% filter(grepl("Politic", all_topics))


category_level<- "topic_level1"
#category_level<- "t1_t2"
#category_level<- "all_topics"

plot_pronoun_by_subreddit = function(df,pronoun,category){
  x_col <- paste("offset_",tolower(pronoun),sep = "")
  y_col <- paste("n50_",tolower(pronoun),sep = "")
   ggplotly(
  ggplot(filter(df, n50_author > 10) ) +
    geom_hline(yintercept = 0) + 
   geom_point(aes_string(x=x_col,y=y_col, size = "n50_author", color=category)) +
    #geom_text(aes_string(x=x_col,y=y_col,label = "subreddit"),size = 2, check_overlap = TRUE)+
  scale_y_continuous(name = paste("Percentage change in the proportion of comments containing", pronoun),limits = c(-50,50)) +
  scale_x_continuous(name = paste("Initial proportion of comments containing", pronoun)) 
  )
}



plot_pronoun_by_subreddit(n50_pronoun_change,"I"   ,category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"Me"  ,category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"We"  ,category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"Us"  ,category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"They",category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"Them",category_level)
plot_pronoun_by_subreddit(n50_pronoun_change,"You" ,category_level)
```




#Aggregate by Category

```{r}
plot_pronoun_by_category = function(df,pronoun,category){
  x_col <- paste("agg_offset_",tolower(pronoun),sep = "")
  y_col <- paste("agg_n50_",tolower(pronoun),sep = "")
   ggplotly(
  ggplot(filter(df, agg_n50_author > 10) ) +
    geom_hline(yintercept = 0) + 
   geom_point(aes_string(x=x_col,y=y_col, size = "agg_n50_author", color=category)) +
  scale_y_continuous(name = paste("Percentage change in the proportion of comments containing", pronoun),limits = c(-50,50)) +
  scale_x_continuous(name = paste("Initial proportion of comments containing", pronoun)) 
  )
}
```

```{r}

n50_pronoun_change <- pronoun_by_post_num %>%
  filter(num_authors > 100, post_num == 49) %>%
  group_by(subreddit) %>%
  summarise(
    n50_i           = mean(ZNcontains_i   , na.rm = TRUE),
    n50_me          = mean(ZNcontains_me  , na.rm = TRUE),
    n50_we          = mean(ZNcontains_we  , na.rm = TRUE),
    n50_us          = mean(ZNcontains_us  , na.rm = TRUE),
    n50_they        = mean(ZNcontains_they, na.rm = TRUE),
    n50_them        = mean(ZNcontains_them, na.rm = TRUE),
    n50_you         = mean(ZNcontains_you , na.rm = TRUE),
    n50_author      = mean(num_authors    , na.rm = TRUE)
  )  %>% ungroup()
agg_n50_pronoun_change  <- pronoun_by_post_num %>%
  filter(post_num == 50) %>%
  left_join(n50_pronoun_change, by = c("subreddit"="subreddit")) %>%
  filter(!is.na(n50_author)) %>%
  ungroup() %>%
  group_by(all_topics) %>% #########HERE
  summarise(
    agg_n50_author = sum(n50_author),
    agg_n50_i      = sum(n50_author*n50_i     )/agg_n50_author,
    agg_n50_me     = sum(n50_author*n50_me    )/agg_n50_author,
    agg_n50_we     = sum(n50_author*n50_we    )/agg_n50_author,
    agg_n50_us     = sum(n50_author*n50_us    )/agg_n50_author,
    agg_n50_they   = sum(n50_author*n50_they  )/agg_n50_author,
    agg_n50_them   = sum(n50_author*n50_them  )/agg_n50_author,
    agg_n50_you    = sum(n50_author*n50_you   )/agg_n50_author,
    
    agg_offset_i      =  sum(n50_author*offset_i   )/agg_n50_author,
    agg_offset_me     =  sum(n50_author*offset_me  )/agg_n50_author,
    agg_offset_we     =  sum(n50_author*offset_we  )/agg_n50_author,
    agg_offset_us     =  sum(n50_author*offset_us  )/agg_n50_author,
    agg_offset_they   =  sum(n50_author*offset_they)/agg_n50_author,
    agg_offset_them   =  sum(n50_author*offset_them)/agg_n50_author,
    agg_offset_you    =  sum(n50_author*offset_you )/agg_n50_author
  ) #%>% filter(grepl("Locations", all_topics)) #########HERE

library(plotly)


category_level<- "topic_level1"
category_level<- "t1_t2"
category_level<- "all_topics"




plot_pronoun_by_category(agg_n50_pronoun_change,"I"   ,category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"Me"  ,category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"We"  ,category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"Us"  ,category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"They",category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"Them",category_level)
plot_pronoun_by_category(agg_n50_pronoun_change,"You" ,category_level)
```


